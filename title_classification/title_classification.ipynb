{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T12:54:36.818669Z",
     "start_time": "2019-03-22T12:54:36.811228Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import textblob\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import mlens\n",
    "from sklearn import neighbors\n",
    "from sklearn.externals import joblib\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from mlens.ensemble import BlendEnsemble\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, RUSBoostClassifier\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T13:33:59.571702Z",
     "start_time": "2019-03-21T13:33:53.659486Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True' #workaround for macOS mkl issue\n",
    "BIG_CATEGORY = 'beauty'\n",
    "# load dataset\n",
    "data_directory = os.path.join(os.path.split(os.getcwd())[0], 'data')\n",
    "prob_dir = os.path.join(data_directory, 'probabilities', BIG_CATEGORY)\n",
    "train = pd.read_csv(os.path.join(data_directory, f'{BIG_CATEGORY}_train_split.csv'))\n",
    "valid = pd.read_csv(os.path.join(data_directory, f'{BIG_CATEGORY}_valid_split.csv'))\n",
    "test = pd.read_csv(os.path.join(data_directory, f'{BIG_CATEGORY}_test_split.csv'))\n",
    "train_x, train_y = train['title'], train['Category']\n",
    "valid_x, valid_y = valid['title'], valid['Category']\n",
    "test_x = test['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T13:34:01.540111Z",
     "start_time": "2019-03-21T13:34:01.536187Z"
    }
   },
   "outputs": [],
   "source": [
    "itemid_train = train['itemid']\n",
    "itemid_valid = valid['itemid']\n",
    "itemid_test = test['itemid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T13:34:23.703861Z",
     "start_time": "2019-03-21T13:34:03.600001Z"
    }
   },
   "outputs": [],
   "source": [
    "train['extractions'] = train['extractions'].map(literal_eval)\n",
    "valid['extractions'] = valid['extractions'].map(literal_eval)\n",
    "test['extractions'] = test['extractions'].map(literal_eval)\n",
    "train['extractions'] = train['extractions'].map(lambda s: ' '.join(s) if s else pd.NaT)\n",
    "valid['extractions'] = valid['extractions'].map(lambda s: ' '.join(s) if s else pd.NaT)\n",
    "test['extractions'] = test['extractions'].map(lambda s: ' '.join(s) if s else pd.NaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:51:51.680875Z",
     "start_time": "2019-03-22T05:50:49.886341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 8), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', strip_accents='unicode',#Stop words may not be needed as they seem to be already removed\n",
    "                             stop_words=None, ngram_range=(1,8))  # \\b[^\\d\\W]{3,}\\b\n",
    "count_vect.fit(train['title'])\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', strip_accents='unicode',\n",
    "                             stop_words=None,) #token_pattern=r'\\b[^\\d\\W]{3,}\\b')\n",
    "tfidf_vect.fit(train['title'])\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', strip_accents='unicode',\n",
    "                                   stop_words=None, ngram_range=(1,8)) \n",
    "tfidf_vect_ngram.fit(train['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T13:35:25.640334Z",
     "start_time": "2019-03-21T13:35:25.634396Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, cross_validate=False,\n",
    "                save_model=False, extract_probs=False, feature_vector_test=None, model_name='sklearn'):\n",
    "    # fit the training dataset on the classifier\n",
    "    #if isinstance(classifier, xgboost.XGBClassifier):\n",
    "    #    feature_vector_train = feature_vector_train.to_csc()\n",
    "    #    feature_vector_valid = feature_vector_valid.to_csc()\n",
    "    if cross_validate:\n",
    "        kfold = model_selection.StratifiedKFold(n_splits=5, random_state=7, shuffle=True)\n",
    "        results = model_selection.cross_val_score(classifier, feature_vector_train, \n",
    "                                                  label, cv=kfold, n_jobs=-1)\n",
    "        print(\"CV Accuracy: %.4f%% (%.4f%%)\" % (results.mean()*100, results.std()*100))\n",
    "        return results.mean()*100\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_train)\n",
    "        print('Train Acc: {}'.format(metrics.accuracy_score(predictions, label)))\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "    if extract_probs:\n",
    "        val_preds = classifier.predict_proba(feature_vector_valid)\n",
    "        test_preds = classifier.predict_proba(feature_vector_test)\n",
    "        print(val_preds.shape)\n",
    "        print(test_preds.shape)\n",
    "        os.makedirs(os.path.join(prob_dir, model_name),exist_ok=True)\n",
    "        np.save(os.path.join(prob_dir, model_name, 'valid.npy'), val_preds)\n",
    "        np.save(os.path.join(prob_dir, model_name, 'test.npy'), test_preds)\n",
    "    if save_model:\n",
    "        model_path = os.path.join(data_directory, 'keras_checkpoints', \n",
    "                                  BIG_CATEGORY, model_name)\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        joblib.dump(classifier, os.path.join(model_path, model_name + '.joblib'))\n",
    "        \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T05:10:53.376244Z",
     "start_time": "2019-03-21T05:10:08.518293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.87346139418841\n",
      "(57317, 17)\n",
      "(76545, 17)\n",
      "NB, Count Vectors:  0.771620985048066\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(make_pipeline(count_vect, naive_bayes.MultinomialNB(alpha=0.25)),\n",
    "                       train_x, train_y, valid_x, cross_validate=False,\n",
    "                       extract_probs=True, feature_vector_test=test_x, model_name='nb_ngrams_2')\n",
    "print(\"NB, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T05:54:34.849918Z",
     "start_time": "2019-03-21T05:46:03.433434Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunyitao/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.8361906989435933\n",
      "(32065, 27)\n",
      "(40417, 27)\n",
      "LR, Count Vectors:  0.7988460938718228\n",
      "Train Acc: 0.5994854402993802\n",
      "(32065, 27)\n",
      "(40417, 27)\n",
      "LR, N-Gram Vectors:  0.6474348978637143\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(make_pipeline(count_vect,\n",
    "                                     linear_model.LogisticRegression(solver='sag', n_jobs=6, multi_class='multinomial',\n",
    "                                                                     tol=1e-4, C=1.e4 / 533292)),\n",
    "                       train_x, train_y, valid_x, cross_validate=False,\n",
    "                save_model=True, extract_probs=True, feature_vector_test=test_x, model_name='log_reg')\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(make_pipeline(tfidf_vect_ngram,\n",
    "                                     linear_model.LogisticRegression(solver='sag', n_jobs=6, multi_class='multinomial',\n",
    "                                                                     tol=1e-4, C=1.e4 / 533292)),\n",
    "                       train_x, train_y, valid_x, cross_validate=False,\n",
    "                save_model=True, extract_probs=True, feature_vector_test=test_x, model_name='log_reg_tfidf')\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-21T10:33:14.279Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = train_model(make_pipeline(count_vect, ensemble.RandomForestClassifier(n_estimators=80, max_depth=580, min_samples_leaf=2)),\n",
    "                       train_x, train_y, valid_x,cross_validate=False,\n",
    "                        save_model=True, extract_probs=True, feature_vector_test=test_x, model_name='rf')\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(make_pipeline(tfidf_vect, ensemble.RandomForestClassifier(n_estimators=80, max_depth=580, min_samples_leaf=2)),\n",
    "                       train_x, train_y, valid_x, cross_validate=False,\n",
    "                       save_model=True, extract_probs=True, feature_vector_test=test_x, model_name='rf_tfidf')\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T11:00:44.324920Z",
     "start_time": "2019-03-22T10:10:11.359063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.7656564863520976\n",
      "(57317, 17)\n",
      "(76545, 17)\n",
      "Xgb,CountVec:  0.7565818169129578\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "xgb = xgboost.XGBClassifier(max_depth=6, learning_rate=0.1, scale_pos_weight=1,\n",
    "                          n_estimators=150, silent=True,\n",
    "                          objective=\"binary:logistic\", booster='gbtree',\n",
    "                          n_jobs=12, nthread=None, gamma=0, min_child_weight=1,\n",
    "                          max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1,\n",
    "                          reg_alpha=0, reg_lambda=1)\n",
    "accuracy = train_model(make_pipeline(count_vect, xgb), train_x, train_y,\n",
    "                       valid_x, cross_validate=False, save_model=True, \n",
    "                       extract_probs=True, feature_vector_test=test_x, \n",
    "                       model_name='xgb')\n",
    "print(\"Xgb,CountVec: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T12:48:22.642242Z",
     "start_time": "2019-03-22T11:56:31.262823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.773983058979526\n",
      "(57317, 17)\n",
      "(76545, 17)\n",
      "Xgb,TFIDF  0.7542439415880106\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier(max_depth=6, learning_rate=0.1, scale_pos_weight=1,\n",
    "                          n_estimators=150, silent=True,\n",
    "                          objective=\"binary:logistic\", booster='gbtree',\n",
    "                          n_jobs=-1, nthread=None, gamma=0, min_child_weight=1,\n",
    "                          max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1,\n",
    "                          reg_alpha=0, reg_lambda=1)\n",
    "accuracy = train_model(make_pipeline(tfidf_vect_ngram, xgb), train_x, train_y, valid_x,\n",
    "                       cross_validate=False, save_model=True, \n",
    "                       extract_probs=True, feature_vector_test=test_x, \n",
    "                       model_name='xgb_tfidf')\n",
    "print(\"Xgb,TFIDF \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:50:30.501506Z",
     "start_time": "2019-03-22T05:29:26.382585Z"
    }
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=80, leaf_size=30)\n",
    "accuracy = train_model(make_pipeline(tfidf_vect_ngram, knn),\n",
    "                       train_x, train_y, valid_x, cross_validate=False, save_model=True, extract_probs=True,\n",
    "                       model_name='knn80_tfidf', feature_vector_test=test_x)\n",
    "print(\"KNN _ count: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:50:30.522288Z",
     "start_time": "2019-03-22T05:29:26.587Z"
    }
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=160, leaf_size=30)\n",
    "accuracy = train_model(make_pipeline(tfidf_vect_ngram, knn),\n",
    "                       train_x, train_y, valid_x, cross_validate=False, save_model=True, extract_probs=True,\n",
    "                       model_name='knn160_tfidf', feature_vector_test=test_x)\n",
    "print(\"KNN _ count: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train Acc: 0.7695209930822713\n",
    "(57317, 17)\n",
    "(76545, 17)\n",
    "KNN _ count:  0.7393443480991678\n",
    "    KNN10 tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-22T12:54:49.773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunyitao/anaconda3/lib/python3.6/site-packages/mlens/parallel/_base_functions.py:226: MetricWarning: [multinomialnb.0.1] Could not score multinomialnb. Details:\n",
      "ValueError(\"Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\",)\n",
      "  (name, inst_name, exc), MetricWarning)\n",
      "/Users/sunyitao/anaconda3/lib/python3.6/site-packages/mlens/parallel/_base_functions.py:226: MetricWarning: [logisticregression.0.1] Could not score logisticregression. Details:\n",
      "ValueError(\"Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\",)\n",
      "  (name, inst_name, exc), MetricWarning)\n",
      "/Users/sunyitao/anaconda3/lib/python3.6/site-packages/mlens/parallel/_base_functions.py:226: MetricWarning: [xgbclassifier.0.1] Could not score xgbclassifier. Details:\n",
      "ValueError(\"Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\",)\n",
      "  (name, inst_name, exc), MetricWarning)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': [9, 11, 13],\n",
    "    #'learning_rate': [0.05, 0.1, 0.2],\n",
    "    #'n_estimators': range(50, 200, 50),\n",
    "    #'gamma': [i/10.0 for i in range(0, 5)],\n",
    "    #'subsample': [i/10.0 for i in range(6, 10)],\n",
    "    #'colsample_bytree': [i/10.0 for i in range(6, 10)],\n",
    "    #'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "ensemble = BlendEnsemble(scorer=accuracy_score, random_state=42, verbose=2)\n",
    "ensemble.add([\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=58*10, min_samples_leaf=10),  \n",
    "    LogisticRegression(solver='sag', n_jobs=12, multi_class='multinomial', tol=1e-4, C=1.e4 / 533292),\n",
    "    naive_bayes.MultinomialNB(alpha=0.25),\n",
    "    xgboost.XGBClassifier(max_depth=6, learning_rate=0.1, scale_pos_weight=1,\n",
    "                          n_estimators=150, silent=True,\n",
    "                          objective=\"binary:logistic\", booster='gbtree',\n",
    "                          n_jobs=12, nthread=None, gamma=0, min_child_weight=1,\n",
    "                          max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1,\n",
    "                          reg_alpha=0, reg_lambda=1),\n",
    "], proba=True)\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(LogisticRegression(solver='sag', n_jobs=12, multi_class='multinomial',\n",
    "                                     tol=1e-4, C=1.e4 / 533292))\n",
    "\n",
    "\n",
    "accuracy = train_model(make_pipeline(tfidf_vect_ngram, ensemble),\n",
    "                                     train_x, train_y, valid_x,\n",
    "                                  cross_validate=False, save_model=True, \n",
    "                                   extract_probs=True, feature_vector_test=test_x, \n",
    "                                   model_name='blend_ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
